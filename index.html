<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Object Detection: Project Architecture</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="doxyfile.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Object Detection
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Project Architecture </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_doxyfile__readme"></a> You can find our <b>best models</b> in dev branch, <em>models/yolo-object-lane-unfroze/weights/</em>.</p>
<p>Training a Yolo Object detection model, with <b>Lane detection (segmentation)</b> as well.</p>
<div class="image">
<img src="Fluxograma.jpg" alt="" width="60%"/>
<div class="caption">
Project Structure</div></div>
<h1><a class="anchor" id="autotoc_md1"></a>
Inference Result</h1>
<div class="image">
<img src="val_batch1_pred.jpg" alt="" width="50%"/>
<div class="caption">
Results</div></div>
<h1><a class="anchor" id="autotoc_md2"></a>
Datasets</h1>
<p>You can download our dataset here: <a href="https://drive.google.com/drive/folders/1RwFmYyjxCafdnUORBcm2kgo62itcLmcS?usp=drive_link">Click to access our dataset</a>.</p>
<p>For lane and drivable area detection, we took some images of our lab and also used the bdd10k dataset. For the objects, we downloaded some datasets from Roboflow.</p>
<p>I noticed a decay of object detection outside the objects of my dataset, despite the fact that i froze the backbone and lowered the learning rate. So you might need to add some object images to preserve more information.</p>
<h2><a class="anchor" id="autotoc_md3"></a>
Roboflow</h2>
<p>To create the masks of our lab images, we used Roboflow to generate the annotations, then we added blur, noise and grayscale to the images and resized them to 320x320, keeping aspect ratio. After this, we downloaded to <b>COCO segmentation</b> format. This creates a Json file with all the annotations. So, in <em>scripts/json_txt.py</em> we convert these annotations, that have the polygons of the lanes into binary masks, so we can then convert them back to normalized polygons and save them to a txt file. We tried to convert the COCO polygons directly to yolo-seg txt format but we noticed some weird values and loss of information. Therefore, converting first to binary masks helped preserve the lane shape.</p>
<p>We downloaded a few Roboflow datasets of road objects, such as the <b>traffic lights, the stop, speed, danger and cross walk signs</b>. We then used <em>scripts/remap_polygons.py</em> to reorder the labels to match our project and <em><a class="el" href="bbox__seg_8py.html">scripts/bbox_seg.py</a></em> to transform them into segmentation labels, since this is a segmentation model.</p>
<h1><a class="anchor" id="autotoc_md4"></a>
Creating Annotations</h1>
<p>In the scripts directory file <em><a class="el" href="create__annotations_8py.html">create_annotations.py</a></em> we create the <b>annotation labels</b> for lane and object detection. How do we do this? We pass our images through a pre-trained <b>Yolo11-seg model</b>, to get the object polygons. Then, we use <b>supervision</b> tools to convert the lane <b>binary masks</b> to valid Yolo <b>polygons</b>. Finaly, we <b>merge</b> the object and lane annotations and get the label files.</p>
<p>Be attentive towards the size of the images and masks, we decided to keep the images square, (training and testing), for compatibility. In the scripts directory, file <em><a class="el" href="resize_8py.html">resize.py</a></em> you can resize images with <b>letterboxing</b> (keeping <b>aspect ratio</b>), or not.</p>
<p>In these scripts, you might need to change some function <b>parameters</b>, the original size of the images, and the <b>paths</b> to the images, so that it correctly links to your dataset and original size of your images.</p>
<p>For debugging, you can <b>visualize the annotations</b> in <em><a class="el" href="visual__annotations_8py.html">scripts/visual_annotations.py</a></em>.</p>
<h1><a class="anchor" id="autotoc_md5"></a>
Training and Testing</h1>
<p>In <em><a class="el" href="training_8py.html">training.py</a></em> (scripts directory) where we are retraining our model, we set the augmentations to None since it disrupts our images, and add other augmentations that dont disrupt them, such as brightness, saturation and hue. After the first training where we freeze the backbone, you train again to unfreeze everything.</p>
<p>For testing, (in <em><a class="el" href="testing_8py.html">scripts/testing.py</a></em>), we call our trained model and set it to <b>predict</b>, to test the prediction of a given validation image.</p>
<p>For debugging we added <em><a class="el" href="count__labels_8py.html">scripts/count_labels.py</a></em> that outputs how many annotations we have for each class id.</p>
<h1><a class="anchor" id="autotoc_md6"></a>
Jetson Nano</h1>
<h2><a class="anchor" id="autotoc_md7"></a>
YOLO Inference on Jetson Nano â€“ Performance Improvements and Benchmarks</h2>
<p>This project documents the key improvements and testing results when running various versions of YOLO on the Jetson Nano. The main goal was to maximize the inference speed (FPS) while using limited hardware resources.</p>
<hr  />
<h3><a class="anchor" id="autotoc_md9"></a>
Key Improvements</h3>
<h3><a class="anchor" id="autotoc_md10"></a>
1. Upgrading to Ubuntu 20.04</h3>
<p>To unlock better compatibility and performance, the default OS was upgraded to <b>Ubuntu 20.04</b>, following this image and guide by Qengineering:</p>
<p>ðŸ‘‰ <a href="https://github.com/Qengineering/Jetson-Nano-Ubuntu-20-image">Qengineering Jetson Nano Ubuntu 20.04 Image</a></p>
<p>This enabled smoother installation of recent dependencies, CUDA support, and improved system stability.</p>
<hr  />
<h3><a class="anchor" id="autotoc_md12"></a>
2. Converting YOLO to TensorRT</h3>
<p>The second major improvement was converting YOLO models to <b>TensorRT</b> using the <code>tensorrtx</code> repository:</p>
<p>ðŸ‘‰ <a href="https://github.com/wang-xinyu/tensorrtx/tree/yolov5-v7.0">Tensorrtx YOLOv5 Conversion (v7.0)</a></p>
<p>By doing so, we achieved significantly higher inference performance using the Jetson's GPU and Tensor cores, particularly with small and segmentation models.</p>
<hr  />
<h3><a class="anchor" id="autotoc_md14"></a>
Benchmark Results</h3>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Environment   </th><th class="markdownTableHeadNone">YOLO Version   </th><th class="markdownTableHeadNone">Frames/s   </th><th class="markdownTableHeadNone">Type cam   </th><th class="markdownTableHeadNone">Framework    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Container Ultralytics (Jetson nano 18.04)   </td><td class="markdownTableBodyNone">v11 nano   </td><td class="markdownTableBodyNone">6.94   </td><td class="markdownTableBodyNone">USB   </td><td class="markdownTableBodyNone">Pytorch    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Container Ultralytics (Jetson nano 18.04)   </td><td class="markdownTableBodyNone">v8 nano   </td><td class="markdownTableBodyNone">12.66   </td><td class="markdownTableBodyNone">USB   </td><td class="markdownTableBodyNone">Pytorch    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Jetson nano Ubuntu 20.04   </td><td class="markdownTableBodyNone">v11 nano   </td><td class="markdownTableBodyNone">12.05   </td><td class="markdownTableBodyNone">USB   </td><td class="markdownTableBodyNone">Pytorch    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Jetson nano Ubuntu 20.04   </td><td class="markdownTableBodyNone">v8 nano   </td><td class="markdownTableBodyNone">14.08   </td><td class="markdownTableBodyNone">USB   </td><td class="markdownTableBodyNone">Pytorch    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Jetson nano Ubuntu 20.04   </td><td class="markdownTableBodyNone">v8 nano   </td><td class="markdownTableBodyNone">15.47   </td><td class="markdownTableBodyNone">CSI   </td><td class="markdownTableBodyNone">Pytorch    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><b>Jetson nano Ubuntu 20.04</b>   </td><td class="markdownTableBodyNone"><b>v5 small</b>   </td><td class="markdownTableBodyNone"><b>47</b>   </td><td class="markdownTableBodyNone">CSI   </td><td class="markdownTableBodyNone"><b>CPP and TensorRT</b>    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Jetson nano Ubuntu 20.04   </td><td class="markdownTableBodyNone">v5 small seg   </td><td class="markdownTableBodyNone">37   </td><td class="markdownTableBodyNone">CSI   </td><td class="markdownTableBodyNone">CPP and TensorRT    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><b>Jetson nano Ubuntu 20.04</b>   </td><td class="markdownTableBodyNone"><b>v8 nano seg</b>   </td><td class="markdownTableBodyNone"><b>57</b>   </td><td class="markdownTableBodyNone">Images   </td><td class="markdownTableBodyNone"><b>CPP and TensorRT</b>   </td></tr>
</table>
<hr  />
<h3><a class="anchor" id="autotoc_md16"></a>
Summary</h3>
<ul>
<li>Upgrading to <b>Ubuntu 20.04</b> greatly improved system support and performance.</li>
<li>Converting models to <b>TensorRT</b> delivered <b>3x to 4x speedup</b> in FPS.</li>
<li>The best result was with <b>YOLOv8 nano segmentation (TensorRT)</b> at <b>57 FPS</b> using preloaded images.</li>
<li><b>CSI cameras</b> outperform USB in most live camera inference scenarios.</li>
</ul>
<hr  />
<h3><a class="anchor" id="autotoc_md18"></a>
Recommendations</h3>
<ul>
<li>For maximum performance: use <b>TensorRT + CSI camera</b>.</li>
<li>For segmentation tasks: consider using YOLOv5 or v8 nano models with <code>tensorrtx</code>.</li>
<li>Avoid using Docker unless strictly needed, as native Ubuntu 20.04 offers better access to CUDA/TensorRT features. </li>
</ul>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
